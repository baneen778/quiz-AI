{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baneen778/quiz-AI/blob/main/Lab_Exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Quiz-2**"
      ],
      "metadata": {
        "id": "wKBOgALplcmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform following tasks on the provided Reviews Dataset.\n",
        "* Drop words if not alphabets.\n",
        "* Tokenize the sentence.\n",
        "* Perform lemitization.\n",
        "* Vectorize using bigram and trigram techniques.\n",
        "* Apply Random Forest algorithm with 150 trees.\n",
        "* Evaluate overall accuracy of the model and class-wise precision ."
      ],
      "metadata": {
        "id": "m5VxQWNuli1L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1xqIwagUf6Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "stopwords.words('english')\n",
        "exclude = string.punctuation"
      ],
      "metadata": {
        "id": "v6RWVYb0a1rU",
        "outputId": "f6c11624-18f6-4de9-bcf1-ad649e5eace3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load dataset\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# ## **Quiz-2**\n",
        "# Perform following tasks on the provided Reviews Dataset.\n",
        "# * Drop words if not alphabets.\n",
        "# * Tokenize the sentence.\n",
        "# * Perform lemitization.\n",
        "# * Vectorize using bigram and trigram techniques.\n",
        "# * Apply Random Forest algorithm with 150 trees.\n",
        "# * Evaluate overall accuracy of the model and class-wise precision .\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt') # Fixed download name\n",
        "\n",
        "\n",
        "stopwords.words('english')\n",
        "exclude = string.punctuation\n",
        "\n",
        "# Load the dataset (replace 'your_file.csv' with the actual file name)\n",
        "try:\n",
        "    df = pd.read_csv('/content/reviews_dataset.csv') # Make sure the file is uploaded to your colab environment\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: '/content/reviews_dataset.csv' not found. Please upload the dataset to your Colab environment.\")\n",
        "    df = pd.DataFrame() # Create an empty DataFrame to avoid further errors"
      ],
      "metadata": {
        "id": "V1xuaDUOa8vM",
        "outputId": "8910e7d1-a80c-4c71-a572-562276c730c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Check column names to find the correct name\n",
        "print(df.columns)\n",
        "\n",
        "# Replace 'review_column' with the actual column name\n",
        "# Update this line with the correct column name from your output above.\n",
        "actual_column_name = 'news' # Replace 'news' with the actual column name if different\n",
        "\n",
        "# Define a function to drop non-alphabetic words\n",
        "def clean_reviews(review):\n",
        "    # Use regex to filter out non-alphabetic words\n",
        "    return \" \".join([word for word in review.split() if word.isalpha()])\n",
        "\n",
        "# Apply the function to the dataset\n",
        "df['cleaned_reviews'] = df[actual_column_name].apply(clean_reviews)\n",
        "\n",
        "# Preview the cleaned data\n",
        "print(\"\\nAfter cleaning:\")\n",
        "print(df[[actual_column_name, 'cleaned_reviews']].head())\n",
        "\n",
        "# Save the cleaned dataset (optional)\n",
        "df.to_csv('cleaned_reviews_dataset.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "rBnGBteObV5h",
        "outputId": "b7401d25-680b-4e6b-d1bf-519e90fff6b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['news', 'type'], dtype='object')\n",
            "\n",
            "After cleaning:\n",
            "                                                news  \\\n",
            "0  China had role in Yukos split-up\\n \\n China le...   \n",
            "1  Oil rebounds from weather effect\\n \\n Oil pric...   \n",
            "2  Indonesia 'declines debt freeze'\\n \\n Indonesi...   \n",
            "3  $1m payoff for former Shell boss\\n \\n Shell is...   \n",
            "4  US bank in $515m SEC settlement\\n \\n Five Bank...   \n",
            "\n",
            "                                     cleaned_reviews  \n",
            "0  China had role in Yukos China lent Russia to h...  \n",
            "1  Oil rebounds from weather effect Oil prices re...  \n",
            "2  Indonesia debt Indonesia no longer needs the d...  \n",
            "3  payoff for former Shell boss Shell is to pay t...  \n",
            "4  US bank in SEC settlement Five Bank of America...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "metadata": {
        "id": "FQEH5gEVemHN",
        "outputId": "0d4c98d4-bf84-4d10-9d60-f24ab18087a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "nQ6W03qle4We",
        "outputId": "6fef8b80-693b-4dde-d4fb-e3344377aa19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-24-56ceacfdca67>, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-56ceacfdca67>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TOKENIZATION TO FIRST TWO SENTENCES\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "stopwords.words('english')\n",
        "exclude = string.punctuation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt') # Fixed download name\n",
        "\n",
        "\n",
        "stopwords.words('english')\n",
        "exclude = string.punctuation\n",
        "\n",
        "# Load the dataset (replace 'your_file.csv' with the actual file name)\n",
        "try:\n",
        "    df = pd.read_csv('/content/reviews_dataset.csv') # Make sure the file is uploaded to your colab environment\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: '/content/reviews_dataset.csv' not found. Please upload the dataset to your Colab environment.\")\n",
        "    df = pd.DataFrame() # Create an empty DataFrame to avoid further errors\n",
        "\n",
        "\n",
        "# Check column names to find the correct name\n",
        "print(df.columns)\n",
        "\n",
        "# Replace 'review_column' with the actual column name\n",
        "# Update this line with the correct column name from your output above.\n",
        "actual_column_name = 'news' # Replace 'news' with the actual column name if different\n",
        "\n",
        "# Define a function to drop non-alphabetic words\n",
        "def clean_reviews(review):\n",
        "    # Use regex to filter out non-alphabetic words\n",
        "    return \" \".join([word for word in review.split() if word.isalpha()])\n",
        "\n",
        "# Apply the function to the dataset\n",
        "df['cleaned_reviews'] = df[actual_column_name].apply(clean_reviews)\n",
        "\n",
        "# Preview the cleaned data\n",
        "print(\"\\nAfter cleaning:\")\n",
        "print(df[[actual_column_name, 'cleaned_reviews']].head())\n",
        "\n",
        "# Save the cleaned dataset (optional)\n",
        "df.to_csv('cleaned_reviews_dataset.csv', index=False)\n",
        "\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Assuming 'df' and 'actual_column_name' are defined from the previous code.\n",
        "# If not, make sure to run the previous code block first.\n",
        "\n",
        "if not df.empty:  # Check if the DataFrame is not empty\n",
        "    try:\n",
        "        # Access the first sentence in the specified column.\n",
        "        first_sentence = df[actual_column_name].iloc[0]\n",
        "        second_sentence = df[actual_column_name].iloc[1]\n",
        "\n",
        "        print(\"First Sentence:\", first_sentence)\n",
        "        print(\"Tokenized first sentence:\", word_tokenize(first_sentence))\n",
        "\n",
        "        print(\"Second Sentence:\", second_sentence)\n",
        "        print(\"Tokenized second sentence:\", word_tokenize(second_sentence))\n",
        "\n",
        "    except IndexError:\n",
        "        print(\"Error: The dataset is empty or the column does not exist.\")\n",
        "else:\n",
        "    print(\"Error: The dataset could not be loaded. Please check the file path and name.\")"
      ],
      "metadata": {
        "id": "Yul_9-oSfelo",
        "outputId": "48cb105a-813a-41f4-9ab2-3a79fb106b9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['news', 'type'], dtype='object')\n",
            "\n",
            "After cleaning:\n",
            "                                                news  \\\n",
            "0  China had role in Yukos split-up\\n \\n China le...   \n",
            "1  Oil rebounds from weather effect\\n \\n Oil pric...   \n",
            "2  Indonesia 'declines debt freeze'\\n \\n Indonesi...   \n",
            "3  $1m payoff for former Shell boss\\n \\n Shell is...   \n",
            "4  US bank in $515m SEC settlement\\n \\n Five Bank...   \n",
            "\n",
            "                                     cleaned_reviews  \n",
            "0  China had role in Yukos China lent Russia to h...  \n",
            "1  Oil rebounds from weather effect Oil prices re...  \n",
            "2  Indonesia debt Indonesia no longer needs the d...  \n",
            "3  payoff for former Shell boss Shell is to pay t...  \n",
            "4  US bank in SEC settlement Five Bank of America...  \n",
            "First Sentence: China had role in Yukos split-up\n",
            " \n",
            " China lent Russia $6bn (Â£3.2bn) to help the Russian government renationalise the key Yuganskneftegas unit of oil group Yukos, it has been revealed.\n",
            " \n",
            " The Kremlin said on Tuesday that the $6bn which Russian state bank VEB lent state-owned Rosneft to help buy Yugansk in turn came from Chinese banks. The revelation came as the Russian government said Rosneft had signed a long-term oil supply deal with China. The deal sees Rosneft receive $6bn in credits from China's CNPC.\n",
            " \n",
            " According to Russian newspaper Vedomosti, these credits would be used to pay off the loans Rosneft received to finance the purchase of Yugansk. Reports said CNPC had been offered 20% of Yugansk in return for providing finance but the company opted for a long-term oil supply deal instead. Analysts said one factor that might have influenced the Chinese decision was the possibility of litigation from Yukos, Yugansk's former owner, if CNPC had become a shareholder. Rosneft and VEB declined to comment. \"The two companies [Rosneft and CNPC] have agreed on the pre-payment for long-term deliveries,\" said Russian oil official Sergei Oganesyan. \"There is nothing unusual that the pre-payment is for five to six years.\"\n",
            " \n",
            " The announcements help to explain how Rosneft, a medium-sized, indebted, and relatively unknown firm, was able to finance its surprise purchase of Yugansk. Yugansk was sold for $9.3bn in an auction last year to help Yukos pay off part of a $27bn bill in unpaid taxes and fines.\n",
            " \n",
            " The embattled Russian oil giant had previously filed for bankruptcy protection in a US court in an attempt to prevent the forced sale of its main production arm. But Yugansk was sold to a little known shell company which in turn was bought by Rosneft. Yukos claims its downfall was punishment for the political ambitions of its founder Mikhail Khodorkovsky. Once the country's richest man, Mr Khodorkovsky is on trial for fraud and tax evasion.\n",
            " \n",
            " The deal between Rosneft and CNPC is seen as part of China's desire to secure long-term oil supplies to feed its booming economy. China's thirst for products such as crude oil, copper and steel has helped pushed global commodity prices to record levels. \"Clearly the Chinese are trying to get some leverage [in Russia],\" said Dmitry Lukashov, an analyst at brokerage Aton. \"They understand property rights in Russia are not the most important rights, and they are more interested in guaranteeing supplies.\" \"If the price of oil is fixed under the deal, which is unlikely, it could be very profitable for the Chinese,\" Mr Lukashov continued. \"And Rosneft is in desperate need of cash, so it's a good deal for them too.\"\n",
            "\n",
            "Tokenized first sentence: ['China', 'had', 'role', 'in', 'Yukos', 'split-up', 'China', 'lent', 'Russia', '$', '6bn', '(', 'Â£3.2bn', ')', 'to', 'help', 'the', 'Russian', 'government', 'renationalise', 'the', 'key', 'Yuganskneftegas', 'unit', 'of', 'oil', 'group', 'Yukos', ',', 'it', 'has', 'been', 'revealed', '.', 'The', 'Kremlin', 'said', 'on', 'Tuesday', 'that', 'the', '$', '6bn', 'which', 'Russian', 'state', 'bank', 'VEB', 'lent', 'state-owned', 'Rosneft', 'to', 'help', 'buy', 'Yugansk', 'in', 'turn', 'came', 'from', 'Chinese', 'banks', '.', 'The', 'revelation', 'came', 'as', 'the', 'Russian', 'government', 'said', 'Rosneft', 'had', 'signed', 'a', 'long-term', 'oil', 'supply', 'deal', 'with', 'China', '.', 'The', 'deal', 'sees', 'Rosneft', 'receive', '$', '6bn', 'in', 'credits', 'from', 'China', \"'s\", 'CNPC', '.', 'According', 'to', 'Russian', 'newspaper', 'Vedomosti', ',', 'these', 'credits', 'would', 'be', 'used', 'to', 'pay', 'off', 'the', 'loans', 'Rosneft', 'received', 'to', 'finance', 'the', 'purchase', 'of', 'Yugansk', '.', 'Reports', 'said', 'CNPC', 'had', 'been', 'offered', '20', '%', 'of', 'Yugansk', 'in', 'return', 'for', 'providing', 'finance', 'but', 'the', 'company', 'opted', 'for', 'a', 'long-term', 'oil', 'supply', 'deal', 'instead', '.', 'Analysts', 'said', 'one', 'factor', 'that', 'might', 'have', 'influenced', 'the', 'Chinese', 'decision', 'was', 'the', 'possibility', 'of', 'litigation', 'from', 'Yukos', ',', 'Yugansk', \"'s\", 'former', 'owner', ',', 'if', 'CNPC', 'had', 'become', 'a', 'shareholder', '.', 'Rosneft', 'and', 'VEB', 'declined', 'to', 'comment', '.', '``', 'The', 'two', 'companies', '[', 'Rosneft', 'and', 'CNPC', ']', 'have', 'agreed', 'on', 'the', 'pre-payment', 'for', 'long-term', 'deliveries', ',', \"''\", 'said', 'Russian', 'oil', 'official', 'Sergei', 'Oganesyan', '.', '``', 'There', 'is', 'nothing', 'unusual', 'that', 'the', 'pre-payment', 'is', 'for', 'five', 'to', 'six', 'years', '.', \"''\", 'The', 'announcements', 'help', 'to', 'explain', 'how', 'Rosneft', ',', 'a', 'medium-sized', ',', 'indebted', ',', 'and', 'relatively', 'unknown', 'firm', ',', 'was', 'able', 'to', 'finance', 'its', 'surprise', 'purchase', 'of', 'Yugansk', '.', 'Yugansk', 'was', 'sold', 'for', '$', '9.3bn', 'in', 'an', 'auction', 'last', 'year', 'to', 'help', 'Yukos', 'pay', 'off', 'part', 'of', 'a', '$', '27bn', 'bill', 'in', 'unpaid', 'taxes', 'and', 'fines', '.', 'The', 'embattled', 'Russian', 'oil', 'giant', 'had', 'previously', 'filed', 'for', 'bankruptcy', 'protection', 'in', 'a', 'US', 'court', 'in', 'an', 'attempt', 'to', 'prevent', 'the', 'forced', 'sale', 'of', 'its', 'main', 'production', 'arm', '.', 'But', 'Yugansk', 'was', 'sold', 'to', 'a', 'little', 'known', 'shell', 'company', 'which', 'in', 'turn', 'was', 'bought', 'by', 'Rosneft', '.', 'Yukos', 'claims', 'its', 'downfall', 'was', 'punishment', 'for', 'the', 'political', 'ambitions', 'of', 'its', 'founder', 'Mikhail', 'Khodorkovsky', '.', 'Once', 'the', 'country', \"'s\", 'richest', 'man', ',', 'Mr', 'Khodorkovsky', 'is', 'on', 'trial', 'for', 'fraud', 'and', 'tax', 'evasion', '.', 'The', 'deal', 'between', 'Rosneft', 'and', 'CNPC', 'is', 'seen', 'as', 'part', 'of', 'China', \"'s\", 'desire', 'to', 'secure', 'long-term', 'oil', 'supplies', 'to', 'feed', 'its', 'booming', 'economy', '.', 'China', \"'s\", 'thirst', 'for', 'products', 'such', 'as', 'crude', 'oil', ',', 'copper', 'and', 'steel', 'has', 'helped', 'pushed', 'global', 'commodity', 'prices', 'to', 'record', 'levels', '.', '``', 'Clearly', 'the', 'Chinese', 'are', 'trying', 'to', 'get', 'some', 'leverage', '[', 'in', 'Russia', ']', ',', \"''\", 'said', 'Dmitry', 'Lukashov', ',', 'an', 'analyst', 'at', 'brokerage', 'Aton', '.', '``', 'They', 'understand', 'property', 'rights', 'in', 'Russia', 'are', 'not', 'the', 'most', 'important', 'rights', ',', 'and', 'they', 'are', 'more', 'interested', 'in', 'guaranteeing', 'supplies', '.', \"''\", '``', 'If', 'the', 'price', 'of', 'oil', 'is', 'fixed', 'under', 'the', 'deal', ',', 'which', 'is', 'unlikely', ',', 'it', 'could', 'be', 'very', 'profitable', 'for', 'the', 'Chinese', ',', \"''\", 'Mr', 'Lukashov', 'continued', '.', '``', 'And', 'Rosneft', 'is', 'in', 'desperate', 'need', 'of', 'cash', ',', 'so', 'it', \"'s\", 'a', 'good', 'deal', 'for', 'them', 'too', '.', \"''\"]\n",
            "Second Sentence: Oil rebounds from weather effect\n",
            " \n",
            " Oil prices recovered in Asian trade on Tuesday, after falling in New York on milder winter weather across the US.\n",
            " \n",
            " With winter temperatures staying relatively high in the northern US, a barrel of light crude ended Monday down $1.33 to $42.12. However crude prices have rebounded in Asia, rising to $42.30 a barrel for February delivery. In London, trading of Brent crude was suspended for a public holiday, but the price fell to $39.20 in the Far East.\n",
            " \n",
            " With milder temperatures expected to continue in the northern parts of the US over the next few days at least, analysts have said the price of oil may fall further - even if the decline was only temporary. \"Weather has been the Achilles' heel of this market,\" said ABN AMRO analyst John Brady. \"But it is winter in the northeast. Eventually we'll get another cold blast.\" Despite a fall of more than $12 a barrel from the record highs reached in late October, the price of crude oil remains almost 30% higher than year-ago levels. Prices rose last week after militant attacks in Riyadh, the capital of Saudi Arabia, briefly renewed fears that the supply chain might be broken in the world's leading crude exporter. \"The market was panicked but fears essentially evaporated... since there was no follow-up,\" said Deborah White, senior economist for energy at SG Securities in Paris.\n",
            "\n",
            "Tokenized second sentence: ['Oil', 'rebounds', 'from', 'weather', 'effect', 'Oil', 'prices', 'recovered', 'in', 'Asian', 'trade', 'on', 'Tuesday', ',', 'after', 'falling', 'in', 'New', 'York', 'on', 'milder', 'winter', 'weather', 'across', 'the', 'US', '.', 'With', 'winter', 'temperatures', 'staying', 'relatively', 'high', 'in', 'the', 'northern', 'US', ',', 'a', 'barrel', 'of', 'light', 'crude', 'ended', 'Monday', 'down', '$', '1.33', 'to', '$', '42.12', '.', 'However', 'crude', 'prices', 'have', 'rebounded', 'in', 'Asia', ',', 'rising', 'to', '$', '42.30', 'a', 'barrel', 'for', 'February', 'delivery', '.', 'In', 'London', ',', 'trading', 'of', 'Brent', 'crude', 'was', 'suspended', 'for', 'a', 'public', 'holiday', ',', 'but', 'the', 'price', 'fell', 'to', '$', '39.20', 'in', 'the', 'Far', 'East', '.', 'With', 'milder', 'temperatures', 'expected', 'to', 'continue', 'in', 'the', 'northern', 'parts', 'of', 'the', 'US', 'over', 'the', 'next', 'few', 'days', 'at', 'least', ',', 'analysts', 'have', 'said', 'the', 'price', 'of', 'oil', 'may', 'fall', 'further', '-', 'even', 'if', 'the', 'decline', 'was', 'only', 'temporary', '.', '``', 'Weather', 'has', 'been', 'the', 'Achilles', \"'\", 'heel', 'of', 'this', 'market', ',', \"''\", 'said', 'ABN', 'AMRO', 'analyst', 'John', 'Brady', '.', '``', 'But', 'it', 'is', 'winter', 'in', 'the', 'northeast', '.', 'Eventually', 'we', \"'ll\", 'get', 'another', 'cold', 'blast', '.', \"''\", 'Despite', 'a', 'fall', 'of', 'more', 'than', '$', '12', 'a', 'barrel', 'from', 'the', 'record', 'highs', 'reached', 'in', 'late', 'October', ',', 'the', 'price', 'of', 'crude', 'oil', 'remains', 'almost', '30', '%', 'higher', 'than', 'year-ago', 'levels', '.', 'Prices', 'rose', 'last', 'week', 'after', 'militant', 'attacks', 'in', 'Riyadh', ',', 'the', 'capital', 'of', 'Saudi', 'Arabia', ',', 'briefly', 'renewed', 'fears', 'that', 'the', 'supply', 'chain', 'might', 'be', 'broken', 'in', 'the', 'world', \"'s\", 'leading', 'crude', 'exporter', '.', '``', 'The', 'market', 'was', 'panicked', 'but', 'fears', 'essentially', 'evaporated', '...', 'since', 'there', 'was', 'no', 'follow-up', ',', \"''\", 'said', 'Deborah', 'White', ',', 'senior', 'economist', 'for', 'energy', 'at', 'SG', 'Securities', 'in', 'Paris', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "sKQVwgTsfTos"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt:  Perform lemitization\n",
        "\n",
        "# Assuming 'df' and 'actual_column_name' are defined and the necessary NLTK downloads are complete.\n",
        "\n",
        "if not df.empty:\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    def lemmatize_text(text):\n",
        "        tokens = word_tokenize(text)\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        return \" \".join(lemmatized_tokens)\n",
        "\n",
        "    df['lemmatized_reviews'] = df[actual_column_name].apply(lemmatize_text)\n",
        "    print(df[[actual_column_name, 'lemmatized_reviews']].head())\n",
        "else:\n",
        "    print(\"DataFrame is empty. Cannot perform lemmatization.\")"
      ],
      "metadata": {
        "id": "_IxcvG6bf9Vr",
        "outputId": "e8bbaf20-b4c4-4963-d384-b4fb50197993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                news  \\\n",
            "0  China had role in Yukos split-up\\n \\n China le...   \n",
            "1  Oil rebounds from weather effect\\n \\n Oil pric...   \n",
            "2  Indonesia 'declines debt freeze'\\n \\n Indonesi...   \n",
            "3  $1m payoff for former Shell boss\\n \\n Shell is...   \n",
            "4  US bank in $515m SEC settlement\\n \\n Five Bank...   \n",
            "\n",
            "                                  lemmatized_reviews  \n",
            "0  China had role in Yukos split-up China lent Ru...  \n",
            "1  Oil rebound from weather effect Oil price reco...  \n",
            "2  Indonesia 'declines debt freeze ' Indonesia no...  \n",
            "3  $ 1m payoff for former Shell bos Shell is to p...  \n",
            "4  US bank in $ 515m SEC settlement Five Bank of ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "TBFCMPfdhklJ",
        "outputId": "ef9db198-d0c2-4f3a-e125-34410b2f5d39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fd5d031fa285>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Split the data into training and test sets (80% train, 20% test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Initialize CountVectorizer with 2-grams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Vectorize using bigram and trigram techniques.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "if not df.empty:\n",
        "    # Vectorize using bigrams and trigrams\n",
        "    vectorizer = CountVectorizer(ngram_range=(2, 3))  # Consider bigrams and trigrams\n",
        "    X = vectorizer.fit_transform(df['lemmatized_reviews'])\n",
        "\n",
        "\n",
        "    print(X.shape)\n",
        "else:\n",
        "    print(\"DataFrame is empty. Cannot perform vectorization.\")"
      ],
      "metadata": {
        "id": "J-G-WSgTkl_f",
        "outputId": "e73c544f-3ace-4df5-f1fc-228a4b9c4aa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2225, 927127)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "\n",
        "if not df.empty:\n",
        "    target_column = 'news'\n",
        "    if target_column not in df.columns:\n",
        "        print(f\"Error: Target column '{target_column}' not found in the DataFrame.\")\n",
        "    else:\n",
        "        y = df[target_column]\n",
        "        X = df.drop(columns=[target_column])\n",
        "        # **Convert categorical features to numerical using Label Encoding**\n",
        "        for column in X.select_dtypes(include=['object']).columns:\n",
        "            label_encoder = LabelEncoder()\n",
        "            X[column] = label_encoder.fit_transform(X[column])\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Initialize and train Random Forest Classifier\n",
        "        rf_classifier = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
        "        rf_classifier.fit(X_train, y_train)  # Train the model\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "        # Evaluate model performance\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')  # For multi-class precision\n",
        "\n",
        "        print(f\"Overall Accuracy: {accuracy}\")\n",
        "        print(f\"Weighted Precision: {precision}\")\n",
        "else:\n",
        "    print(\"DataFrame is empty. Cannot perform model training and evaluation.\")\n"
      ],
      "metadata": {
        "id": "dP3dRKsQlGYg",
        "outputId": "97e0ac17-05bb-4366-9daa-a7be52a5c733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.060674157303370786\n",
            "Weighted Precision: 0.054307116104868915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}